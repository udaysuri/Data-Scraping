{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d6a191a",
   "metadata": {},
   "source": [
    "# Leje PDF/Image Scraper\n",
    "#### Made By- Uday S.\n",
    "#### For- Raphael G."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abbd96f",
   "metadata": {},
   "source": [
    "## I. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1a3311c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing drivers\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "#importing beautiful soup\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#importing the necessary libraries\n",
    "import pandas as pd, numpy as np\n",
    "#importing json package to read the data\n",
    "import json\n",
    "\n",
    "#importing os module\n",
    "import os\n",
    "from os.path import basename\n",
    "#importing time\n",
    "import time\n",
    "#importing pathlib\n",
    "import pathlib\n",
    "#importing string\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae37256b",
   "metadata": {},
   "source": [
    "## II. Defining Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0f1e5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining a function to find the position of a substring's nth instance in a string\n",
    "def find_nth(haystack, needle, n):\n",
    "    start = haystack.find(needle)\n",
    "    while start >= 0 and n > 1:\n",
    "        start = haystack.find(needle, start+len(needle))\n",
    "        n -= 1\n",
    "    return start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ca79e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url= \"https://www.leje.com.br\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d975fb14",
   "metadata": {},
   "source": [
    "## III. Scraping Page Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cf7caaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Selenium to define driver\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "#Defining base url\n",
    "url= \"https://www.leje.com.br/\"\n",
    "#Opening url\n",
    "driver.get(url)\n",
    "#Defining an empty variable\n",
    "p_db=\"\"\n",
    "#getting the required element containing page links by class_name\n",
    "p_element = driver.find_elements_by_class_name('auction-card-box ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c20a6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating an empty DataFrame\n",
    "url_df= pd.DataFrame()\n",
    "#Creating an empty \"url\" column with the required rows\n",
    "url_df[\"url\"]= [None] * len(p_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab3815e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using for loop to get the links of all the required pages\n",
    "for i in range(len(p_element)):\n",
    "    #getting the string having the link using get_attribute(), and assigning it to the column\n",
    "    url_df[\"url\"][i]=str(p_element[i].get_attribute(\"onclick\"))\n",
    "    #Cleaning the string to get the exact link for the page using find_nth function defined above\n",
    "    url_df[\"url\"][i]=url+url_df[\"url\"][i][find_nth(url_df[\"url\"][i],\"index\",1):find_nth(url_df[\"url\"][i],\"'\",2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b79c2d6",
   "metadata": {},
   "source": [
    "## IV. Defining Columns, Scraping Listing Names & Defining Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3b3b81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the Name column\n",
    "url_df[\"Name\"]= \"\"\n",
    "#Defining the Name xpath to scrape data from\n",
    "Name = '//*[@id=\"auction-title-text\"]/p'\n",
    "#Defining Drivers\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "#Using for loop to iterate through the pages\n",
    "for i in range(len(p_element)):\n",
    "    #Opening the url\n",
    "    driver.get(url_df[\"url\"][i])\n",
    "    #Using Xpath to get the required element with listing name\n",
    "    name_element= driver.find_element_by_xpath(Name)\n",
    "    #Using .text to get the listing name and assigning to the url_df[\"Name\"] column\n",
    "    url_df[\"Name\"][i]=name_element.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50b403df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining Parent Directory\n",
    "parent_dir = os.getcwd()\n",
    "# Directory\n",
    "directory = \"Leje\"\n",
    "# Directory paths\n",
    "path = os.path.join(parent_dir, directory)\n",
    "# Create the directory\n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b33f0a",
   "metadata": {},
   "source": [
    "## V. Scraping PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a82cdff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(len(url_df)):\n",
    "    # URL from which pdfs to be downloaded\n",
    "    url = url_df[\"url\"][n]\n",
    "\n",
    "    # Requests URL and get response object\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse text obtained\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all hyperlinks present on webpage\n",
    "    links = soup.find_all('a')\n",
    "\n",
    "    i = 0\n",
    "    \n",
    "    \n",
    "    d1= url_df[\"Name\"][n].replace(\"/\",\" \").replace('\\ ',\" \")\n",
    "    # Directory paths\n",
    "    path1= os.path.join(path, d1)\n",
    "\n",
    "    # Create the directory\n",
    "    if not os.path.exists(path1):\n",
    "        os.mkdir(path1)\n",
    "    else:\n",
    "        pass\n",
    "    # From all links check for pdf link and\n",
    "    # if present download file\n",
    "    for link in links:\n",
    "        if ('.pdf' in link.text):\n",
    "            i += 1\n",
    "            print(\"Downloading file: \", n,\": \",i)\n",
    "\n",
    "            # Get response object for link\n",
    "            response = requests.get(base_url+link.get('href'))\n",
    "            # Write content in pdf file\n",
    "            if not os.path.exists(directory+\"/\"+d1+\"/\"+link.text.replace(\"\\n\",\"\").replace(\"\\t\",\"\")):\n",
    "                                                    pdf = open(directory+\"/\"+d1+\"/\"+link.text.replace(\"\\n\",\"\").replace(\"\\t\",\"\"), 'wb')\n",
    "                                                    pdf.write(response.content)\n",
    "                                                    pdf.close()\n",
    "                                                    print(\"File \", n,\": \",i, \" downloaded\")\n",
    "            else:\n",
    "                                                   pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8b7dba",
   "metadata": {},
   "source": [
    "## VI. Scraping Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "194a13f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(len(url_df)):\n",
    "    # URL from which pngs to be downloaded\n",
    "    url = url_df[\"url\"][n]\n",
    "\n",
    "    # Requests URL and get response object\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse text obtained\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all hyperlinks present on webpage\n",
    "    links = soup.find_all('img')\n",
    "\n",
    "    i = 0\n",
    "\n",
    "\n",
    "    d1= url_df[\"Name\"][n].replace(\"/\",\" \").replace('\\ ',\" \")\n",
    "    # Directory paths\n",
    "    path1= os.path.join(path, d1)\n",
    "\n",
    "    # Create the directory\n",
    "    if not os.path.exists(path1):\n",
    "        os.mkdir(path1)\n",
    "    else:\n",
    "        pass\n",
    "    # From all links check for jpeg link and\n",
    "    # if present download file\n",
    "    for link in links:\n",
    "        if \"imagem\" in str(link):\n",
    "            try:\n",
    "                i += 1\n",
    "\n",
    "                print(\"Downloading file: \", n,\": \",i)\n",
    "                # Get response object for link\n",
    "                response = requests.get(base_url+link.get(\"src\"))\n",
    "                # Write content in png file\n",
    "                png = open(directory+\"/\"+d1+\"/\"+str(i)+\".png\", 'wb')\n",
    "                png.write(response.content)\n",
    "                png.close()\n",
    "                print(\"File \", n,\": \",i, \" downloaded\")\n",
    "            except:\n",
    "                pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
