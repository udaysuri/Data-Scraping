{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d6a191a",
   "metadata": {},
   "source": [
    "# Megaleiloes Data Scraper\n",
    "#### Made By- Uday S.\n",
    "#### For- Raphael G."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abbd96f",
   "metadata": {},
   "source": [
    "## I. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "740e4651",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing drivers\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "#importing beautiful soup\n",
    "from bs4 import BeautifulSoup\n",
    "#importing the necessary libraries\n",
    "import pandas as pd, numpy as np\n",
    "#importing json package to read the data\n",
    "import json\n",
    "#importing os module\n",
    "import os\n",
    "from os.path import basename\n",
    "#importing time\n",
    "import time\n",
    "#importing pathlib\n",
    "import pathlib\n",
    "#importing string\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6760809d",
   "metadata": {},
   "source": [
    "## II. Defining Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fd8ab8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining a function to find the position of a substring's nth instance in a string\n",
    "def find_nth(haystack, needle, n):\n",
    "    start = haystack.find(needle)\n",
    "    while start >= 0 and n > 1:\n",
    "        start = haystack.find(needle, start+len(needle))\n",
    "        n -= 1\n",
    "    return start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "591fd6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining xpaths\n",
    "xpath_name= \"/html/body/div[3]/div[2]/div[2]/div[2]/div/h1\"\n",
    "xpath_name_alt= \"/html/body/div[3]/div[3]/div[2]/div[2]/div/h1\"\n",
    "xpath_main= \"/html/body/div[3]/div[3]/div[2]/div[2]/div\"\n",
    "xpath_main_alt= \"/html/body/div[3]/div[2]/div[2]/div[2]/div\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e51820",
   "metadata": {},
   "source": [
    "## III. Scraping Page Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59369dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Selenium to define driver\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "#defining the base url\n",
    "url= \"https://www.megaleiloes.com.br/imoveis\"\n",
    "#opening the url\n",
    "driver.get(url)\n",
    "#defining the variable to be used in the for loop\n",
    "p_db=\"\"\n",
    "#iterating through the pages using a for loop to get the links of all the listings\n",
    "for i in range(13):\n",
    "    time.sleep(5) #waiting for the webpage to load\n",
    "    p_element = driver.find_element_by_xpath('//*[@id=\"w0\"]/div[1]') #getting the required element\n",
    "    p_db= p_db + \" \" +p_element.get_attribute('innerHTML')# Getting attributes of an element\n",
    "    #using try & except so that an error doesn't come up if pages are less than 13\n",
    "    try:\n",
    "        click_element = driver.find_element_by_xpath('//*[@id=\"w0\"]/div[2]/ul/li[11]/a')\n",
    "        click_element.click()\n",
    "    except:\n",
    "        pass\n",
    "#Creating an empty DataFrame\n",
    "url_df= pd.DataFrame()\n",
    "#Creating an empty \"url\" column with the required rows\n",
    "url_df[\"url\"]= [None] * 584\n",
    "#Using a for loop to add the listing's link to the appropriate row in the \"url\" column\n",
    "for i in range(584):\n",
    "    try:\n",
    "        url_df[\"url\"][i]= p_db[find_nth(p_db,'class=\"card-title\" href=\"',i+1)+25:find_nth(p_db,'\" data-pjax=\"0\">',(3*i)+1)] #Using .find() to index the strings in order to get the link in the correct form\n",
    "        a=i\n",
    "    except:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91399836",
   "metadata": {},
   "source": [
    "## IV. Pages' Data Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aada82c",
   "metadata": {},
   "source": [
    "### IV. A) Defining Df Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57a28318",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining empty columns\n",
    "url_df[\"Name\"]=\"\"\n",
    "url_df[\"Location\"]=\"\" \n",
    "url_df[\"1st Square Date\"]=\"\"\n",
    "url_df[\"Evaluation Value 1ª\"]=\"\"\n",
    "url_df[\"2nd Square Date\"]=\"\"\n",
    "url_df[\"Valuation Value 2nd\"]=\"\"\n",
    "url_df[\"Visits\"]=\"\"\n",
    "url_df[\"Description\"]=\"\"\n",
    "url_df[\"Purchase and sale contract\"]=\"\"\n",
    "url_df[\"Vara\"]=\"\"\n",
    "url_df[\"Forum\"]=\"\"\n",
    "url_df[\"Auctioneer\"]=\"\"\n",
    "url_df[\"Author\"]=\"\"\n",
    "url_df[\"Defendant\"]=\"\"\n",
    "url_df[\"Process number\"]=\"\"\n",
    "url_df[\"Site Process\"]=\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe12a13",
   "metadata": {},
   "source": [
    "### IV. B) Scraping Data Onto Respective Df Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2026e3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Selenium to define driver\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "#Using for loop to iterate through the links\n",
    "for i in range(584):\n",
    "    driver.get(url_df[\"url\"][i]) #opening the link\n",
    "    time.sleep(1) #waiting for the webpage to load\n",
    "    #Using try/except to skip any exception due to different data structure for name_element\n",
    "    try:\n",
    "        name_element= driver.find_element_by_xpath(xpath_name)\n",
    "    except:\n",
    "        name_element= driver.find_element_by_xpath(xpath_name_alt)\n",
    "    #Using try/except to skip any exception due to missing second instance date & value\n",
    "    try:\n",
    "        seconddate_element= driver.find_elements_by_class_name('card-second-instance-date')[0]\n",
    "        firstdate_element= driver.find_elements_by_class_name('card-first-instance-date')[0]\n",
    "        firstprice_element= driver.find_elements_by_class_name('card-instance-value')[0] \n",
    "        secondprice_element= driver.find_elements_by_class_name('card-instance-value')[1]\n",
    "    except:\n",
    "        firstdate_element= driver.find_elements_by_class_name('card-first-instance-date')[1]\n",
    "        firstprice_element= driver.find_elements_by_class_name('card-instance-value')[0]\n",
    "        seconddate_element=\"\"\n",
    "        secondprice_element=\"\"\n",
    "    visits_element= driver.find_element_by_class_name(\"views-count\")\n",
    "    desc_element= driver.find_element_by_xpath('//*[@id=\"tab-description\"]/div')\n",
    "    contract_element= driver.find_element_by_xpath('//*[@id=\"tab-contract\"]/div')\n",
    "    #Using try/except to skip any exception due to different data structure for the main data description elements\n",
    "    try:\n",
    "        vara_element= driver.find_element_by_xpath(xpath_main)\n",
    "        forum_element= driver.find_element_by_xpath(xpath_main)\n",
    "        leiloeiro_element= driver.find_element_by_xpath(xpath_main)\n",
    "        author_element= driver.find_element_by_xpath(xpath_main)\n",
    "        def_element= driver.find_element_by_xpath(xpath_main)\n",
    "        num_element= driver.find_element_by_xpath(xpath_main)\n",
    "        site_element= driver.find_element_by_xpath(xpath_main)\n",
    "        loc_element= driver.find_element_by_xpath(xpath_main)\n",
    "    except:\n",
    "        vara_element= driver.find_element_by_xpath(xpath_main_alt)\n",
    "        forum_element= driver.find_element_by_xpath(xpath_main_alt)\n",
    "        leiloeiro_element= driver.find_element_by_xpath(xpath_main_alt)\n",
    "        author_element= driver.find_element_by_xpath(xpath_main_alt)\n",
    "        def_element= driver.find_element_by_xpath(xpath_main_alt)\n",
    "        num_element= driver.find_element_by_xpath(xpath_main_alt)\n",
    "        site_element= driver.find_element_by_xpath(xpath_main_alt)\n",
    "        loc_element= driver.find_element_by_xpath(xpath_main_alt)\n",
    "    #Getting the attribute/text from the elements defined above\n",
    "    url_df[\"Name\"][i]=name_element.get_attribute('innerHTML')\n",
    "    url_df[\"1st Square Date\"][i]= firstdate_element.text\n",
    "    url_df[\"Evaluation Value 1ª\"][i]= firstprice_element.text\n",
    "    try:\n",
    "        url_df[\"2nd Square Date\"][i]= seconddate_element.text\n",
    "        url_df[\"Valuation Value 2nd\"][i]= secondprice_element.text\n",
    "    except:\n",
    "        pass\n",
    "    url_df[\"Location\"][i]= loc_element.get_attribute('innerHTML')\n",
    "    url_df[\"Visits\"][i]= visits_element.get_attribute('innerHTML')\n",
    "    url_df[\"Description\"][i]= desc_element.get_attribute('innerHTML')\n",
    "    url_df[\"Purchase and sale contract\"][i]= contract_element.get_attribute('innerHTML')\n",
    "    url_df[\"Vara\"][i]= vara_element.get_attribute('innerHTML')\n",
    "    url_df[\"Forum\"][i]= forum_element.get_attribute('innerHTML')\n",
    "    url_df[\"Auctioneer\"][i]= leiloeiro_element.get_attribute('innerHTML')\n",
    "    url_df[\"Author\"][i]= author_element.get_attribute('innerHTML')\n",
    "    url_df[\"Defendant\"][i]= def_element.get_attribute('innerHTML')\n",
    "    url_df[\"Process number\"][i]= num_element.get_attribute('innerHTML')\n",
    "    url_df[\"Site Process\"][i]= site_element.get_attribute('innerHTML')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12d699c",
   "metadata": {},
   "source": [
    "### V. Cleaning Data in the Df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a44ab2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using a for loop to iterate through all the rows in the dataframe\n",
    "for i in range(584):\n",
    "    #cleaning the Location data using find_nth function defined above\n",
    "    url_df[\"Location\"][i]= url_df[\"Location\"][i][find_nth(url_df[\"Location\"][i],'Localização',1)+65:find_nth(url_df[\"Location\"][i][find_nth(url_df[\"Location\"][i],'Localização',1)+65:],'</div',1)+find_nth(url_df[\"Location\"][i],'Localização',1)+65]\n",
    "    #cleaning the Location data using replace() & strip()\n",
    "    url_df[\"1st Square Date\"][i]=url_df[\"1st Square Date\"][i].replace(\"1ª Praça: \",\"\").strip(\"Data: \") \n",
    "    #cleaning the data using replace()\n",
    "    url_df[\"2nd Square Date\"][i]=url_df[\"2nd Square Date\"][i].replace(\"2ª Praça: \",\"\") \n",
    "    #cleaning the Location data using find_nth function defined above\n",
    "    url_df[\"Visits\"][i]=url_df[\"Visits\"][i][find_nth(url_df[\"Visits\"][i],'\"value\">',1)+8:][:find_nth(url_df[\"Visits\"][i][find_nth(url_df[\"Visits\"][i],'\"value\">',1)+8:],'</',1)]\n",
    "    #cleaning the Location data using find_nth function defined above, and strip()\n",
    "    url_df[\"Description\"][i]=url_df[\"Description\"][i].strip('\\n        ')[:find_nth(url_df[\"Description\"][i].strip('\\n        '),'<b',1)]\n",
    "    #cleaning the Location data using strip() & replace()\n",
    "    url_df[\"Purchase and sale contract\"][i]=url_df[\"Purchase and sale contract\"][i].replace(\"<br>\",\"\").replace(\"\\n\",\"\").strip(\" \")\n",
    "    #cleaning the Location data using find_nth function defined above, and find()\n",
    "    url_df[\"Vara\"][i]=url_df[\"Vara\"][i][url_df[\"Vara\"][i].find(\"Vara\"):][url_df[\"Vara\"][i][url_df[\"Vara\"][i].find(\"Vara\"):].find(\"value\")+7:find_nth(url_df[\"Vara\"][i][url_df[\"Vara\"][i].find(\"Vara\"):],\"</d\",2)]\n",
    "    #cleaning the Location data using find_nth function defined above, and find()\n",
    "    url_df[\"Forum\"][i]=url_df[\"Forum\"][i][url_df[\"Forum\"][i].find(\"Forum\"):][url_df[\"Forum\"][i][url_df[\"Forum\"][i].find(\"Forum\"):].find(\"value\")+7:find_nth(url_df[\"Forum\"][i][url_df[\"Forum\"][i].find(\"Forum\"):],\"</d\",2)]\n",
    "    #cleaning the Location data using find_nth function defined above, find(), and strip()\n",
    "    url_df[\"Auctioneer\"][i]=url_df[\"Auctioneer\"][i][url_df[\"Auctioneer\"][i].find(\"Leiloeiro\"):][url_df[\"Auctioneer\"][i][url_df[\"Auctioneer\"][i].find(\"Leiloeiro\"):].find(\"value\")+7:][:url_df[\"Auctioneer\"][i][url_df[\"Auctioneer\"][i].find(\"Leiloeiro\"):][url_df[\"Auctioneer\"][i][url_df[\"Auctioneer\"][i].find(\"Leiloeiro\"):].find(\"value\")+7:].find(\"</\")].strip(\"\\n\").strip(\" \").strip(\"\\n\")\n",
    "    #cleaning the Location data using find_nth function defined above, and find()\n",
    "    url_df[\"Author\"][i]=url_df[\"Author\"][i][url_df[\"Author\"][i].find(\"Autor\"):][url_df[\"Author\"][i][url_df[\"Author\"][i].find(\"Autor\"):].find(\"value\")+7:find_nth(url_df[\"Author\"][i][url_df[\"Author\"][i].find(\"Autor\"):],\"</d\",2)]\n",
    "    #cleaning the Location data using find_nth function defined above, and find()\n",
    "    url_df[\"Defendant\"][i]=url_df[\"Defendant\"][i][url_df[\"Defendant\"][i].find(\"Réu\"):][url_df[\"Defendant\"][i][url_df[\"Defendant\"][i].find(\"Réu\"):].find(\"value\")+7:find_nth(url_df[\"Defendant\"][i][url_df[\"Defendant\"][i].find(\"Réu\"):],\"</d\",2)]\n",
    "    #cleaning the Location data using find_nth function defined above, and find()\n",
    "    url_df[\"Site Process\"][i]=url_df[\"Site Process\"][i][url_df[\"Site Process\"][i].find(\"https://esaj\"):][:url_df[\"Site Process\"][i][url_df[\"Site Process\"][i].find(\"https://esaj\"):].find('\"')]\n",
    "    #using if-else statement to assign the correct value to the \"Process number\" column based on \"Site Process\" column's value\n",
    "    if url_df[\"Site Process\"][i] == \"\":\n",
    "        url_df[\"Process number\"][i]= \"\"\n",
    "    else:\n",
    "        url_df[\"Process number\"][i]=url_df[\"Process number\"][i][url_df[\"Process number\"][i].find('\"top\"')+6:][:url_df[\"Process number\"][i][url_df[\"Process number\"][i].find('\"top\"')+6:].find(\" ()\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e5b986",
   "metadata": {},
   "source": [
    "### VI. Downloading The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f89f204a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downloading the url_df to an excel file\n",
    "url_df.to_excel(\"Mgaleiloes.xlsx\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
